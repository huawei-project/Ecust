[net]
height=112
width=96
channels=3


# layer 0: ConvBlock
[convolutional]
filters=64
size=3
stride=2
padding=1
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=64


# layer 3: ConvBlock
[convolutional]
filters=64
size=3
stride=1
padding=1
groups=64
activation=linear

[batchnorm]
bias=1

[prelu]
n=64


# layer 6: Bottleneck
[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=128
size=3
stride=2
padding=1
groups=128
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=64
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1


# layer 14: Bottleneck
[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=128
size=3
stride=1
padding=1
groups=128
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=64
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=13
activation=linear

# layer 23: Bottleneck
[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=128
size=3
stride=1
padding=1
groups=128
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=64
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=22
activation=linear

# layer 32: Bottleneck
[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=128
size=3
stride=1
padding=1
groups=128
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=64
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=31
activation=linear

# layer 41: Bottleneck
[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=128
size=3
stride=1
padding=1
groups=128
activation=linear

[batchnorm]
bias=1

[prelu]
n=128

[convolutional]
filters=64
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=40
activation=linear

# layer 50: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=2
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1


# layer 58: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=57
activation=linear

# layer 67: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=66
activation=linear

# layer 76: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=75
activation=linear

# layer 85: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=84
activation=linear

# layer 94: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=93
activation=linear

# layer 103: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=102
activation=linear

# layer 112: Bottleneck
[convolutional]
filters=512
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=512

[convolutional]
filters=512
size=3
stride=2
padding=1
groups=512
activation=linear

[batchnorm]
bias=1

[prelu]
n=512

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1


# layer 120: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=119
activation=linear

# layer 129: Bottleneck
[convolutional]
filters=256
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=256
size=3
stride=1
padding=1
groups=256
activation=linear

[batchnorm]
bias=1

[prelu]
n=256

[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[shortcut]
from=128
activation=linear

# layer 138: ConvBlock
[convolutional]
filters=512
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

[prelu]
n=512


# layer 141: ConvBlock
[connected-locally]
output=512
groups=512
activation=linear

[batchnorm]
bias=1


# layer 144: ConvBlock
[convolutional]
filters=128
size=1
stride=1
padding=0
groups=1
activation=linear

[batchnorm]
bias=1

